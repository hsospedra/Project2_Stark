# Vanguard A/B Test Analysis

## üß™ Project Overview

This project analyzes the results of a digital A/B experiment conducted by Vanguard to evaluate whether a redesigned online user interface improves client experience and process completion. The experiment compares a traditional interface (Control group) with a redesigned version (Test group).

The analysis focuses on measuring differences between the Test and Control groups using key performance indicators (KPI) such as completion rate, time spent across process steps, and error-related behavior. These metrics are used to assess whether the redesigned interface leads to a more efficient and intuitive client journey.

In addition to the core experiment evaluation, the project includes exploratory analyses of client demographics and behavior. These analyses examine relationships between client tenure, number of accounts, balances, engagement metrics, and support interactions in order to provide additional context around client behavior and potential secondary effects of the redesign.

---

## üõ†Ô∏è Tools
- Python (pandas, numpy, scipy, matplotlib, seaborn)
- SQL
- Tableau
- Jupyter Notebook
- Git & GitHub

---
## üìÅ Repository Structure

The repository contains the following folders and files:

###  data_raw
Contains the original datasets provided for the project. These raw files are kept unchanged for reference and reproducibility.

###  data_clean
Includes cleaned and processed datasets used for analysis. The cleaning steps applied to the raw data are documented in the notebooks.

###  figures
Contains visual assets generated during the project, including charts from exploratory data analysis, diagrams created in Miro, and Entity Relationship Diagrams (ERD) used for SQL analysis and database understanding.

###  notebook
Holds the analysis notebooks used in the project. These notebooks contain the code for data cleaning, exploratory data analysis, metric calculations, hypothesis testing, and visualizations.

## üìÖ Project Log

### Day 1 ‚Äì Dataset Discovery & Initial EDA
- Loaded all datasets into Python.
- Explored dataset structure, data types, and key variables.
- Reviewed metadata to understand client demographics and behavior.
- Performed initial exploratory analysis using pandas.
- Identified potential data quality issues.

---

### Day 2 ‚Äì Data Cleaning & Client Analysis
- Cleaned datasets and addressed missing or inconsistent values.
- Merged datasets required for analysis.
- Analyzed client demographics to identify primary users of the process.
- Compared client age groups and tenure (new vs long-standing clients).
- Conducted initial client behavior analysis.

---

### Day 3 ‚Äì Performance Metrics Definition
- Reviewed KPI and metrics concepts.
- Defined key success metrics for the experiment.
- Calculated completion rates for Test and Control groups.
- Analyzed time spent on each step of the process.
- Evaluated error rates based on backward navigation.
- Compared performance between the new and old designs.

---

### Day 4 ‚Äì Hypothesis Testing 
- Defined hypotheses related to completion rate differences.
- Conducted statistical tests to evaluate significance.
- Began analysis of whether the observed increase met the 5% threshold.
- Selected an additional hypothesis to test.
- Started evaluating experiment design effectiveness.

---

### Day 5 ‚Äì Hypothesis Testing & Experiment Evaluation
- Completed hypothesis testing analyses.
- Completed additional hypothesis testing.
- Evaluated experiment duration and limitations.
- Identified additional data that could improve the analysis.

---

### Day 6 ‚Äì Tableau Metrics & Data Preparation
- Defined metrics to be visualized in Tableau.
- Prepared and exported cleaned datasets for Tableau.
- Imported data into Tableau.
- Planned dashboard structure based on KPIs.

---

### Day 7 ‚Äì Tableau Dashboard Development
- Built Tableau dashboards showing A/B test results.
- Visualized completion rates, time spent, and error rates.
- Added demographic filters (age, gender).
- Integrated EDA visuals to provide context.
- Refined dashboards for clarity and storytelling.

---

### Day 8 ‚Äì Project Refinement & Bonus Tasks
- Reviewed all previous analyses and outputs.
- Cleaned and organized notebooks and code.
- Addressed optional bonus tasks where applicable.
- Reviewed self-guided lessons on Streamlit and project organization.

---

### Day 9 ‚Äì Presentation Preparation
- Created project presentation following provided guidelines.
- Structured slides to clearly communicate insights.
- Prepared visual and analytical narratives.
- Ensured all deliverables were complete and consistent.

---

### Day 10 ‚Äì Final Review & Presentation
- Performed final checks against project requirements.
- Reviewed README and repository structure.
- Delivered the final presentation.
- Submitted all required links and materials.

---

